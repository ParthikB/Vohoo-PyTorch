{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cats and dogs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParthikB/Vohoo-PyTorch/blob/master/cats_and_dogs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3Yn5L2xfQgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I07yCeyjyQ-J",
        "colab_type": "text"
      },
      "source": [
        "# PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRuUKjerd1db",
        "colab_type": "code",
        "outputId": "900bf5a7-76ed-4c6e-a76d-7cd92e599966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "                                  # VARIABLES #\n",
        "DATA_EXTRACTED = True\n",
        "DATA_CREATED   = True\n",
        "DATA_BALANCED  = True\n",
        "CLIP           = 1680  #Clip every feature lenght at CLIP is data is not balanced.\n",
        "#____________________________________________________________________________________#\n",
        "                              \n",
        "                              # DOWNLOADING DATA #\n",
        "PATH = '/content/drive/My Drive/Colab Notebooks/pyTorch/data/'\n",
        "os.chdir(PATH)\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"parthikb\" # username from the json file \n",
        "os.environ['KAGGLE_KEY'] = \"079b1e8e73bd390b39218acc15c82b09\" # key from the json file\n",
        "!kaggle datasets download -d chetankv/dogs-cats-images\n",
        "#____________________________________________________________________________________#\n",
        "                             \n",
        "                              # EXTRACTING DATA #\n",
        "if not DATA_EXTRACTED:\n",
        "  from zipfile import ZipFile\n",
        "\n",
        "  file_name = 'dogs-cats-images.zip'\n",
        "\n",
        "  with ZipFile(file_name, 'r') as zip:\n",
        "    print('Initiating Extraction...')\n",
        "    zip.extractall()\n",
        "    print('Done!')\n",
        "#____________________________________________________________________________________#\n",
        "                               \n",
        "                                # MOVING DATA #\n",
        "# PATH += 'dog vs cat/dataset'\n",
        "# os.chdir(PATH)\n",
        "\n",
        "# To move the dataset folders...\n",
        "\n",
        "# import shutil\n",
        "# shutil.move(PATH+'/training_set', '/content/drive/My Drive/Colab Notebooks/pyTorch/data/dog vs cat/')\n",
        "# shutil.move(PATH+'/test_set', '/content/drive/My Drive/Colab Notebooks/pyTorch/data/dog vs cat/')\n",
        "#____________________________________________________________________________________#\n",
        "                          \n",
        "                           # CREATING TRAINING SET #\n",
        "PATH = '/content/drive/My Drive/Colab Notebooks/pyTorch/data/dog vs cat/training_set'\n",
        "os.chdir(PATH)\n",
        "\n",
        "class DogsVSCats:\n",
        "  IMG_SIZE     = 50\n",
        "  CATS         = os.path.join(PATH, 'cats') \n",
        "  DOGS         = os.path.join(PATH, 'dogs')\n",
        "  labels       = {CATS:0, DOGS:1}\n",
        "  training_data = []\n",
        "  cat_count, dog_count = 0, 0\n",
        "\n",
        "  def createTrainingData(self):\n",
        "    try:\n",
        "      for label in self.labels:\n",
        "        os.chdir(label)\n",
        "        data = os.listdir()\n",
        "        for image in tqdm(data):\n",
        "          img = os.path.join(label, image)\n",
        "          img = cv2.imread(img, 0)\n",
        "\n",
        "          one_hot = np.eye(len(self.labels))[self.labels[label]]\n",
        "          img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
        "          \n",
        "          self.training_data.append([np.array(img), one_hot])\n",
        "          \n",
        "          if label == self.CATS:\n",
        "            self.cat_count += 1\n",
        "          else:\n",
        "            self.dog_count += 1\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "\n",
        "    np.random.shuffle(self.training_data)\n",
        "    np.save('/content/drive/My Drive/Colab Notebooks/pyTorch/data/dog vs cat/training_data.npy', self.training_data)\n",
        "    print('Cats : ', self.cat_count)\n",
        "    print('Dogs : ', self.dog_count)\n",
        "    \n",
        "if not DATA_CREATED:\n",
        "  print('Creating Training set...')\n",
        "  dogsVScats = DogsVSCats()\n",
        "  dogsVScats.createTrainingData()\n",
        "\n",
        "\n",
        "training_data = np.load('/content/drive/My Drive/Colab Notebooks/pyTorch/data/dog vs cat/training_data.npy', allow_pickle=True)\n",
        "#____________________________________________________________________________________#\n",
        "                              \n",
        "                              # BALANCING TRAINING SET #\n",
        "if not DATA_BALANCED:\n",
        "  print('Balancing Training set...')\n",
        "  cat_counter, dog_counter = 0, 0\n",
        "  balanced_training_data = []\n",
        "\n",
        "  for data in tqdm(training_data):\n",
        "    if data[1][0] == 1 and cat_counter < CLIP: #if cat\n",
        "      cat_counter += 1\n",
        "      balanced_training_data.append(data)\n",
        "    elif data[1][1] == 1 and dog_counter < CLIP: #if dog\n",
        "      dog_counter += 1\n",
        "      balanced_training_data.append(data)\n",
        "\n",
        "  training_data = balanced_training_data\n",
        "#____________________________________________________________________________________#\n",
        "\n",
        "print(\"Length of Final Training Set :\", len(training_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dogs-cats-images.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Length of Final Training Set : 5680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv5wy_j91Ca3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFvtKsAA9b-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    '''\n",
        "    Network Structure:\n",
        "\n",
        "    input > \n",
        "    (1)Conv2D > (2)MaxPool2D > \n",
        "    (3)Conv2D > (4)MaxPool2D > \n",
        "    (5)Conv2D > (6)MaxPool2D > \n",
        "    (7)Linear > (8)LinearOut\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Creating the convulutional Layers\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5)\n",
        "\n",
        "    self.flatten = None\n",
        "    # Creating a Random dummy sample to get the Flattened Dimensions\n",
        "    x = torch.randn(50, 50).view(-1, 1, 50, 50)\n",
        "    x = self.convs(x)\n",
        "\n",
        "    # x = torch.flatten(x)\n",
        "    # print(x.shape)\n",
        "\n",
        "    # Creating the Linear Layers\n",
        "    self.fc1   = nn.Linear(self.flatten, 512)\n",
        "    self.fc2   = nn.Linear(512, 2)\n",
        "\n",
        "    # x = self.forward(x)\n",
        "\n",
        "  def convs(self, x):\n",
        "\n",
        "    # Creating the MaxPooling Layers\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=(2, 2))\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=(2, 2))\n",
        "    x = F.max_pool2d(F.relu(self.conv3(x)), kernel_size=(2, 2))\n",
        "\n",
        "    if not self.flatten:\n",
        "      self.flatten = x[0].shape[0] * x[0].shape[1] * x[0].shape[2]\n",
        "    return x\n",
        "\n",
        "  # FORWARD PASS\n",
        "  def forward(self, x):\n",
        "    x = self.convs(x)\n",
        "    x = x.view(-1, self.flatten)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.softmax(self.fc2(x), dim=1)\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUgDxYmpPPfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Net()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}